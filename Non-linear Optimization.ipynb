{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bc324b",
   "metadata": {},
   "source": [
    "<h1>Non-linear Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6685140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "from math import sin\n",
    "from math import cos\n",
    "from scipy.misc import derivative\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1994a0a",
   "metadata": {},
   "source": [
    "<b>Question 1: Write a code to find the root of the function, that is, find x where the value of the following function is 0.</b> (10 points) <br>\n",
    "Take the equation ùêπ(ùë•)=ùë•‚àísin(ùë•).\n",
    "\n",
    "Note that ùêπ‚Ä≤(ùë•)=1‚àícos(ùë•). Apply Newton's method with ùë•0=1 (initial point) and $\\epsilon=10^{-5}$ (error should be smaller than this, that is F(x) < $\\epsilon$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a34bff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f,Df,x0,epsilon,max_iter):\n",
    "    xn = x0\n",
    "    for n in range(0,max_iter):\n",
    "        fxn = f(xn)\n",
    "        if abs(fxn) < epsilon:\n",
    "            print('Found solution after',n,'iterations! \\n')\n",
    "            return print('The solution is',xn)\n",
    "        Dfxn = Df(xn)\n",
    "        if Dfxn == 0:\n",
    "            print('Zero derivative. No solution found.')\n",
    "            return None\n",
    "        xn = xn - fxn/Dfxn\n",
    "        print('Checking for x =',xn,'f(x) =',fxn, 'f\\'(x) =',Dfxn)\n",
    "    print('Exceeded maximum iterations. No solution found.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36b1e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for x = 0.6551450720424304 f(x) = 0.1585290151921035 f'(x) = 0.45969769413186023\n",
      "Checking for x = 0.43359036836349285 f(x) = 0.045870786040895783 f'(x) = 0.2070404522188285\n",
      "Checking for x = 0.2881484008925013 f(x) = 0.013458737959401001 f'(x) = 0.09253682546673025\n",
      "Checking for x = 0.19183231215063873 f(x) = 0.0039709484604172895 f'(x) = 0.041228298535459174\n",
      "Checking for x = 0.12780966756070838 f(x) = 0.0011743969232932971 f'(x) = 0.018343461611362577\n",
      "Checking for x = 0.08518323360286417 f(x) = 0.00034768434920498525 f'(x) = 0.008156543180431908\n",
      "Checking for x = 0.05678195278661648 f(x) = 0.00010298015675494487 f'(x) = 0.003625898332586197\n",
      "Checking for x = 0.03785260078110906 f(x) = 3.0507717045387406e-05 f'(x) = 0.001611661985920665\n",
      "Found solution after 8 iterations! \n",
      "\n",
      "The solution is 0.03785260078110906\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x - sin(x)\n",
    "Df = lambda x: 1 - cos(x)\n",
    "\n",
    "newton(f,Df,1,1e-5,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c383d2c",
   "metadata": {},
   "source": [
    "<h1>Multi-Variable Optimization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22552a7",
   "metadata": {},
   "source": [
    "Consider the function \\\\[f(x_1,x_2) = 2x_1^3+6x_1x_2^2 -3x_2^3-150x_1. \\\\]\n",
    "\n",
    "<b>Question 2. Write a code to find the optimal point (optimal solutions) for the 2-variable function. The gradient and the hessian is shown below.</b>  (10 points) <br>\n",
    "\n",
    "<b>Note</b> that when calculating -g/H in python, you can use a function such that direction = np.linalg.solve(H, -g) \n",
    "\n",
    "Then the gradient is \\\\[ \\nabla f(x) = \\left( \\begin{array}{c} 6x_1^2+6x_2^2-150 \\\\ 12x_1x_2-9x_2^2 \\end{array} \\right), \\\\]\n",
    "and the Hessian is  \\\\[\\nabla^2 f(x) = \\left( \\begin{array}{cc} 12x_1 & 12x_2\\\\12x_2 & 12x_1-18x_2\\end{array} \\right). \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "336c6aeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numdifftools\n",
      "  Downloading numdifftools-0.9.40-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.8 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from numdifftools) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.9 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from numdifftools) (1.20.1)\n",
      "Collecting algopy>=0.4\n",
      "  Downloading algopy-0.5.7.zip (189 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m189.5/189.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: statsmodels>=0.6 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from numdifftools) (0.12.2)\n",
      "Requirement already satisfied: patsy>=0.5 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from statsmodels>=0.6->numdifftools) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from statsmodels>=0.6->numdifftools) (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.21->statsmodels>=0.6->numdifftools) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.21->statsmodels>=0.6->numdifftools) (2021.1)\n",
      "Requirement already satisfied: six in /Users/yashasbvn/opt/anaconda3/lib/python3.8/site-packages (from patsy>=0.5->statsmodels>=0.6->numdifftools) (1.15.0)\n",
      "Building wheels for collected packages: algopy\n",
      "  Building wheel for algopy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for algopy: filename=algopy-0.5.7-py3-none-any.whl size=107609 sha256=a906dfccdd75126289f4fd8ce7ae41322b9a5b2e2496479beaddebee9113ff89\n",
      "  Stored in directory: /Users/yashasbvn/Library/Caches/pip/wheels/0d/18/4f/be14421713ec96521183a9f4dc86becb3e6c1bf1b5578a4e57\n",
      "Successfully built algopy\n",
      "Installing collected packages: algopy, numdifftools\n",
      "Successfully installed algopy-0.5.7 numdifftools-0.9.40\n"
     ]
    }
   ],
   "source": [
    "!pip install numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c18975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of starting point is  [-138.    3.]\n",
      "Hessian of starting point is  [[12. 12.]\n",
      " [12. -6.]]\n",
      "x1 = 10, x2 = 10\n",
      "x1 = 14.583333333333389, x2 = 14.583333333333389\n",
      "x1 = 19.166666666666778, x2 = 19.166666666666778\n",
      "x1 = 23.750000000000167, x2 = 23.750000000000167\n",
      "x1 = 28.333333333333556, x2 = 28.333333333333556\n",
      "x1 = 32.91666666666694, x2 = 32.91666666666694\n",
      "x1 = 37.50000000000033, x2 = 37.50000000000033\n",
      "x1 = 42.08333333333371, x2 = 42.08333333333371\n",
      "x1 = 46.6666666666671, x2 = 46.6666666666671\n",
      "x1 = 51.25000000000048, x2 = 51.25000000000048\n",
      "x1 = 55.83333333333387, x2 = 55.83333333333387\n",
      "x1 = 60.416666666667254, x2 = 60.416666666667254\n",
      "x1 = 65.00000000000064, x2 = 65.00000000000064\n",
      "x1 = 69.58333333333402, x2 = 69.58333333333402\n",
      "x1 = 74.16666666666741, x2 = 74.16666666666741\n",
      "x1 = 78.7500000000008, x2 = 78.7500000000008\n",
      "x1 = 83.33333333333418, x2 = 83.33333333333418\n",
      "x1 = 87.91666666666757, x2 = 87.91666666666757\n",
      "x1 = 92.50000000000095, x2 = 92.50000000000095\n",
      "x1 = 97.08333333333434, x2 = 97.08333333333434\n",
      "x1 = 101.66666666666772, x2 = 101.66666666666772\n",
      "x1 = 106.25000000000111, x2 = 106.25000000000111\n",
      "x1 = 110.8333333333345, x2 = 110.8333333333345\n",
      "x1 = 115.41666666666788, x2 = 115.41666666666788\n",
      "x1 = 120.00000000000126, x2 = 120.00000000000126\n",
      "x1 = 124.58333333333465, x2 = 124.58333333333465\n",
      "x1 = 129.16666666666805, x2 = 129.16666666666805\n",
      "x1 = 133.75000000000145, x2 = 133.75000000000145\n",
      "x1 = 138.33333333333485, x2 = 138.33333333333485\n",
      "x1 = 142.91666666666825, x2 = 142.91666666666825\n",
      "x1 = 147.50000000000165, x2 = 147.50000000000165\n",
      "x1 = 152.08333333333505, x2 = 152.08333333333505\n",
      "x1 = 156.66666666666845, x2 = 156.66666666666845\n",
      "x1 = 161.25000000000185, x2 = 161.25000000000185\n",
      "x1 = 165.83333333333525, x2 = 165.83333333333525\n",
      "x1 = 170.41666666666865, x2 = 170.41666666666865\n",
      "x1 = 175.00000000000205, x2 = 175.00000000000205\n",
      "x1 = 179.58333333333545, x2 = 179.58333333333545\n",
      "x1 = 184.16666666666885, x2 = 184.16666666666885\n",
      "x1 = 188.75000000000225, x2 = 188.75000000000225\n",
      "x1 = 193.33333333333564, x2 = 193.33333333333564\n",
      "x1 = 197.91666666666904, x2 = 197.91666666666904\n",
      "x1 = 202.50000000000244, x2 = 202.50000000000244\n",
      "x1 = 207.08333333333584, x2 = 207.08333333333584\n",
      "x1 = 211.66666666666924, x2 = 211.66666666666924\n",
      "x1 = 216.25000000000264, x2 = 216.25000000000264\n",
      "x1 = 220.83333333333604, x2 = 220.83333333333604\n",
      "x1 = 225.41666666666944, x2 = 225.41666666666944\n",
      "x1 = 230.00000000000284, x2 = 230.00000000000284\n",
      "x1 = 234.58333333333624, x2 = 234.58333333333624\n",
      "x1 = 239.16666666666964, x2 = 239.16666666666964\n",
      "x1 = 243.75000000000304, x2 = 243.75000000000304\n",
      "x1 = 248.33333333333644, x2 = 248.33333333333644\n",
      "x1 = 252.91666666666984, x2 = 252.91666666666984\n",
      "x1 = 257.50000000000324, x2 = 257.50000000000324\n",
      "x1 = 262.0833333333366, x2 = 262.0833333333366\n",
      "x1 = 266.66666666667, x2 = 266.66666666667\n",
      "x1 = 271.25000000000335, x2 = 271.25000000000335\n",
      "x1 = 275.8333333333367, x2 = 275.8333333333367\n",
      "x1 = 280.4166666666701, x2 = 280.4166666666701\n",
      "x1 = 285.00000000000347, x2 = 285.00000000000347\n",
      "x1 = 289.58333333333684, x2 = 289.58333333333684\n",
      "x1 = 294.1666666666702, x2 = 294.1666666666702\n",
      "x1 = 298.7500000000036, x2 = 298.7500000000036\n",
      "x1 = 303.33333333333695, x2 = 303.33333333333695\n",
      "x1 = 307.9166666666703, x2 = 307.9166666666703\n",
      "x1 = 312.5000000000037, x2 = 312.5000000000037\n",
      "x1 = 317.08333333333707, x2 = 317.08333333333707\n",
      "x1 = 321.66666666667044, x2 = 321.66666666667044\n",
      "x1 = 326.2500000000038, x2 = 326.2500000000038\n",
      "x1 = 330.8333333333372, x2 = 330.8333333333372\n",
      "x1 = 335.41666666667055, x2 = 335.41666666667055\n",
      "x1 = 340.0000000000039, x2 = 340.0000000000039\n",
      "x1 = 344.5833333333373, x2 = 344.5833333333373\n",
      "x1 = 349.16666666667066, x2 = 349.16666666667066\n",
      "x1 = 353.75000000000404, x2 = 353.75000000000404\n",
      "x1 = 358.3333333333374, x2 = 358.3333333333374\n",
      "x1 = 362.9166666666708, x2 = 362.9166666666708\n",
      "x1 = 367.50000000000415, x2 = 367.50000000000415\n",
      "x1 = 372.0833333333375, x2 = 372.0833333333375\n",
      "x1 = 376.6666666666709, x2 = 376.6666666666709\n",
      "x1 = 381.25000000000426, x2 = 381.25000000000426\n",
      "x1 = 385.83333333333763, x2 = 385.83333333333763\n",
      "x1 = 390.416666666671, x2 = 390.416666666671\n",
      "x1 = 395.0000000000044, x2 = 395.0000000000044\n",
      "x1 = 399.58333333333775, x2 = 399.58333333333775\n",
      "x1 = 404.1666666666711, x2 = 404.1666666666711\n",
      "x1 = 408.7500000000045, x2 = 408.7500000000045\n",
      "x1 = 413.33333333333786, x2 = 413.33333333333786\n",
      "x1 = 417.91666666667123, x2 = 417.91666666667123\n",
      "x1 = 422.5000000000046, x2 = 422.5000000000046\n",
      "x1 = 427.083333333338, x2 = 427.083333333338\n",
      "x1 = 431.66666666667135, x2 = 431.66666666667135\n",
      "x1 = 436.2500000000047, x2 = 436.2500000000047\n",
      "x1 = 440.8333333333381, x2 = 440.8333333333381\n",
      "x1 = 445.41666666667146, x2 = 445.41666666667146\n",
      "x1 = 450.00000000000483, x2 = 450.00000000000483\n",
      "x1 = 454.5833333333382, x2 = 454.5833333333382\n",
      "x1 = 459.1666666666716, x2 = 459.1666666666716\n",
      "x1 = 463.75000000000495, x2 = 463.75000000000495\n",
      "Maxiter Reached: 100\n",
      "x1 = 468.3333333333383\n",
      "x2 = 468.3333333333383\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ea4b669f9fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Maximum number of iterations reached: {maxiter}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimal x1 - {x1}, x2 - {x2}, {message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import autograd as ad\n",
    "from autograd import grad, jacobian\n",
    "\n",
    "def f(params):\n",
    "    # print(params)  # <-- you'll see that params is a NumPy array\n",
    "    x1, x2 = params \n",
    "    return (2*x1**3) + (6*x1*x2**2) - (3*x2**3) - (150*x1)\n",
    "\n",
    "import numdifftools as nd #To get Gradient and Hessian\n",
    "\n",
    "def rosen(xy): \n",
    "    return (xy[0]**2 + xy[1])\n",
    "\n",
    "grad = nd.Gradient(f)([1, 1])\n",
    "hess = nd.Hessian(f)([1,1])\n",
    "print(\"Gradient of starting point is \", grad)\n",
    "print(\"Hessian of starting point is \", hess)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def newton(x1, x2, eps, maxiter=100):\n",
    "    \n",
    "    x_0 = np.array([x1, x2]).reshape((2,1))\n",
    "    i = 0\n",
    "    error = 1e9\n",
    "    \n",
    "    while error > eps and i <= maxiter:\n",
    "\n",
    "        print (f\"x1 = {x_0[0][0]}, x2 = {x_0[1][0]}\")\n",
    "        \n",
    "        grad = nd.Gradient(f)([x1,x2])\n",
    "        hess = nd.Hessian(f)([x1,x2])\n",
    "\n",
    "        #g, H = get_grads(x_t[0][0], x_t[1][0])\n",
    "        x_1 = x_0 - np.linalg.solve(hess, -grad)\n",
    "\n",
    "        error = np.abs(np.sum(x_1 - x_0))\n",
    "\n",
    "        x_0 = x_1\n",
    "        i += 1\n",
    "\n",
    "        if i == 100:\n",
    "            print (f'Maxiter Reached: {i}')\n",
    "            print (f\"x1 = {x_0[0][0]}\")\n",
    "            print (f\"x2 = {x_1[1][0]}\")\n",
    "            break\n",
    "            \n",
    "    if error <= eps:\n",
    "        return x_0[0][0], x_0[1][0], f'Required precision has been reached: {diff} <= {eps}'\n",
    "    else:\n",
    "        return None, f'Maximum number of iterations reached: {maxiter}'\n",
    "\n",
    "x1, x2, message = newton(x1 = 10, x2 = 10, eps=1e-15)\n",
    "\n",
    "print(f\"Optimal x1 - {x1}, x2 - {x2}, {message}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340cca3",
   "metadata": {},
   "source": [
    "### Alternative implementation using in-built functions - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a956135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.00000001e+00 2.87998440e-17]\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as optimize\n",
    "\n",
    "def f(params):\n",
    "    # print(params)  # <-- you'll see that params is a NumPy array\n",
    "    x1, x2 = params \n",
    "    return (2*x1**3) + (6*x1*x2**2) - (3*x2**3) - (150*x1)\n",
    "\n",
    "initial_guess = [0, 0]\n",
    "result = optimize.minimize(f, initial_guess)\n",
    "if result.success:\n",
    "    fitted_params = result.x\n",
    "    print(fitted_params)\n",
    "else:\n",
    "    raise ValueError(result.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b462a5",
   "metadata": {},
   "source": [
    "### Alternative implementation using in-built functions - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f7f7e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 : x1 = 2.0097614267121555, x2 = 0.9780486646366923, f(x) = -276.50056172869307\n",
      "Iteration 2 : x1 = 3.9018204620599035, x2 = -1.5946663158968026, f(x) = -394.77027966056414\n",
      "Iteration 3 : x1 = 5.255299394632168, x2 = 0.25222693142112984, f(x) = -496.05352249704583\n",
      "Iteration 4 : x1 = 4.855855975626878, x2 = 0.018926018096883906, f(x) = -499.3722492140146\n",
      "Iteration 5 : x1 = 5.0084452399850194, x2 = -0.023799604282501635, f(x) = -499.9807973548889\n",
      "Iteration 6 : x1 = 5.000556941037659, x2 = 0.0030179833913260047, f(x) = -499.99971749947133\n",
      "Iteration 7 : x1 = 4.999926082688161, x2 = -7.890015495180206e-06, f(x) = -499.99999983422015\n",
      "Iteration 8 : x1 = 5.000002535221375, x2 = -5.620910260713781e-07, f(x) = -499.99999999979775\n",
      "Iteration 9 : x1 = 4.999999990317311, x2 = -4.9187989979206235e-08, f(x) = -499.99999999999994\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -500.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "Nfeval = 1\n",
    "\n",
    "def rosen(x): \n",
    "    \n",
    "    return (2*x[0]**3) + (6*x[0]*x[1]**2) - (3*x[1]**3) - (150*x[0])\n",
    "\n",
    "def callbackF(Xi):\n",
    "    global Nfeval\n",
    "    print(f'Iteration {Nfeval} : x1 = {Xi[0]}, x2 = {Xi[1]}, f(x) = {rosen(Xi)}')\n",
    "    Nfeval += 1\n",
    "\n",
    "x0 = np.array([1, 1], dtype=np.double)\n",
    "[xopt, fopt, gopt, Bopt, func_calls, grad_calls, warnflg] = \\\n",
    "    fmin_bfgs(rosen, \n",
    "              x0, \n",
    "              callback=callbackF, \n",
    "              maxiter=2000, \n",
    "              full_output=True, \n",
    "              retall=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c507d",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression Using Stochastic Gradient Descent</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189b951",
   "metadata": {},
   "source": [
    "In a logistic regression model, the prediction is given by\n",
    "\n",
    "$p(x_i) = \\frac{1}{1+exp(-f(x))}$ where $f(x)$ is given by:\n",
    "\n",
    "$f(x) = \\theta_0+\\theta_1x_1+\\theta_2x_2...$ <br>\n",
    "where $f(x)$ is a linear function (hence logistic regression is called generalized linear regression)\n",
    "\n",
    "We minimize the negative of maximum likelihood function. Thus the loss function we want to minimize is:<br>\n",
    "$minimize: loss(\\theta) = -\\frac{1}{n}\\sum_i\\bigg( \\big(y_i (ln(p(x_i))\\big)+(1-y_i)(1-ln(p(x_i))\\bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a523556",
   "metadata": {},
   "source": [
    "The gradient of the cost function (in direction $j$ is given by (where m is a batch size, you can chose any batch sixe): <br>\n",
    "$\\frac{d(loss(\\theta_))}{d\\theta} = \\frac{1}{m}\\sum_i \\bigg(y_i-p(x_i)\\bigg)x_i^j$\n",
    "\n",
    "Therefore, steps in direction $j$ is given by:<br>\n",
    "$\\theta_j (n+1) = \\theta_j (n)-\\alpha \\frac{1}{m}\\sum_i \\bigg(y_i-p(x_i)\\bigg)x_i^j$\n",
    "\n",
    "Try deriving this from the equation above. Hint, keep it as $p(x_i)$ form, easier for derivation.\n",
    "\n",
    "Check this link to study how the gradients are calculated:\n",
    "https://www.baeldung.com/cs/gradient-descent-logistic-regression\n",
    "\n",
    "<b>Question 3: Write a code to find the coefficients for $\\theta_1$ and $\\theta_2$ (use at least 10 iterations and find the coefficients after 10 iterations)</b>  (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "141edf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data has been taken from Andrew Ng's course\n",
    "# first 2 columns are the predictor variables and 3rd column is the output variable (0 or 1)\n",
    "# marks in 2 exams and if they got admission or not\n",
    "# in most machine learning problems, it is better to normalize the data for the training set.\n",
    "\n",
    "data = np.array([\n",
    "[34.62365962451697,78.0246928153624,0],\n",
    "[30.28671076822607,43.89499752400101,0],\n",
    "[35.84740876993872,72.90219802708364,0],\n",
    "[60.18259938620976,86.30855209546826,1],\n",
    "[79.0327360507101,75.3443764369103,1],\n",
    "[45.08327747668339,56.3163717815305,0],\n",
    "[61.10666453684766,96.51142588489624,1],\n",
    "[75.02474556738889,46.55401354116538,1],\n",
    "[76.09878670226257,87.42056971926803,1],\n",
    "[84.43281996120035,43.53339331072109,1],\n",
    "[95.86155507093572,38.22527805795094,0],\n",
    "[75.01365838958247,30.60326323428011,0],\n",
    "[82.30705337399482,76.48196330235604,1],\n",
    "[69.36458875970939,97.71869196188608,1],\n",
    "[39.53833914367223,76.03681085115882,0],\n",
    "[53.9710521485623,89.20735013750205,1],\n",
    "[69.07014406283025,52.74046973016765,1],\n",
    "[67.94685547711617,46.67857410673128,0],\n",
    "[70.66150955499435,92.92713789364831,1],\n",
    "[76.97878372747498,47.57596364975532,1],\n",
    "[67.37202754570876,42.83843832029179,0],\n",
    "[89.67677575072079,65.79936592745237,1],\n",
    "[50.534788289883,48.85581152764205,0],\n",
    "[34.21206097786789,44.20952859866288,0],\n",
    "[77.9240914545704,68.9723599933059,1],\n",
    "[62.27101367004632,69.95445795447587,1],\n",
    "[80.1901807509566,44.82162893218353,1],\n",
    "[93.114388797442,38.80067033713209,0],\n",
    "[61.83020602312595,50.25610789244621,0],\n",
    "[38.78580379679423,64.99568095539578,0],\n",
    "[61.379289447425,72.80788731317097,1],\n",
    "[85.40451939411645,57.05198397627122,1],\n",
    "[52.10797973193984,63.12762376881715,0],\n",
    "[52.04540476831827,69.43286012045222,1],\n",
    "[40.23689373545111,71.16774802184875,0],\n",
    "[54.63510555424817,52.21388588061123,0],\n",
    "[33.91550010906887,98.86943574220611,0],\n",
    "[64.17698887494485,80.90806058670817,1],\n",
    "[74.78925295941542,41.57341522824434,0],\n",
    "[34.1836400264419,75.2377203360134,0],\n",
    "[83.90239366249155,56.30804621605327,1],\n",
    "[51.54772026906181,46.85629026349976,0],\n",
    "[94.44336776917852,65.56892160559052,1],\n",
    "[82.36875375713919,40.61825515970618,0],\n",
    "[51.04775177128865,45.82270145776001,0],\n",
    "[62.22267576120188,52.06099194836679,0],\n",
    "[77.19303492601364,70.45820000180959,1],\n",
    "[97.77159928000232,86.7278223300282,1],\n",
    "[62.07306379667647,96.76882412413983,1],\n",
    "[91.56497449807442,88.69629254546599,1],\n",
    "[79.94481794066932,74.16311935043758,1],\n",
    "[99.2725269292572,60.99903099844988,1],\n",
    "[90.54671411399852,43.39060180650027,1],\n",
    "[34.52451385320009,60.39634245837173,0],\n",
    "[50.2864961189907,49.80453881323059,0],\n",
    "[49.58667721632031,59.80895099453265,0],\n",
    "[97.64563396007767,68.86157272420604,1],\n",
    "[32.57720016809309,95.59854761387875,0],\n",
    "[74.24869136721598,69.82457122657193,1],\n",
    "[71.79646205863379,78.45356224515052,1],\n",
    "[75.3956114656803,85.75993667331619,1],\n",
    "[35.28611281526193,47.02051394723416,0],\n",
    "[56.25381749711624,39.26147251058019,0],\n",
    "[30.05882244669796,49.59297386723685,0],\n",
    "[44.66826172480893,66.45008614558913,0],\n",
    "[66.56089447242954,41.09209807936973,0],\n",
    "[40.45755098375164,97.53518548909936,1],\n",
    "[49.07256321908844,51.88321182073966,0],\n",
    "[80.27957401466998,92.11606081344084,1],\n",
    "[66.74671856944039,60.99139402740988,1],\n",
    "[32.72283304060323,43.30717306430063,0],\n",
    "[64.0393204150601,78.03168802018232,1],\n",
    "[72.34649422579923,96.22759296761404,1],\n",
    "[60.45788573918959,73.09499809758037,1],\n",
    "[58.84095621726802,75.85844831279042,1],\n",
    "[99.82785779692128,72.36925193383885,1],\n",
    "[47.26426910848174,88.47586499559782,1],\n",
    "[50.45815980285988,75.80985952982456,1],\n",
    "[60.45555629271532,42.50840943572217,0],\n",
    "[82.22666157785568,42.71987853716458,0],\n",
    "[88.9138964166533,69.80378889835472,1],\n",
    "[94.83450672430196,45.69430680250754,1],\n",
    "[67.31925746917527,66.58935317747915,1],\n",
    "[57.23870631569862,59.51428198012956,1],\n",
    "[80.36675600171273,90.96014789746954,1],\n",
    "[68.46852178591112,85.59430710452014,1],\n",
    "[42.0754545384731,78.84478600148043,0],\n",
    "[75.47770200533905,90.42453899753964,1],\n",
    "[78.63542434898018,96.64742716885644,1],\n",
    "[52.34800398794107,60.76950525602592,0],\n",
    "[94.09433112516793,77.15910509073893,1],\n",
    "[90.44855097096364,87.50879176484702,1],\n",
    "[55.48216114069585,35.57070347228866,0],\n",
    "[74.49269241843041,84.84513684930135,1],\n",
    "[89.84580670720979,45.35828361091658,1],\n",
    "[83.48916274498238,48.38028579728175,1],\n",
    "[42.2617008099817,87.10385094025457,1],\n",
    "[99.31500880510394,68.77540947206617,1],\n",
    "[55.34001756003703,64.9319380069486,1],\n",
    "[74.77589300092767,89.52981289513276,1],\n",
    "])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9d52aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['x1','x2','Y'])\n",
    "df.Y = df.Y.astype(int)\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "scores = df[['x1', 'x2']].values\n",
    "results = df['Y'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6c8745a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def logistic_function(x):    \n",
    "    return 1/ (1 + np.exp(-x))\n",
    "logistic_function(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d841dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(theta, x, y):\n",
    "    m = len(y)\n",
    "    y_pred = logistic_function(np.dot(x , theta))\n",
    "    error = (y * np.log(y_pred)) + ((1 - y) * np.log(1 - y_pred))\n",
    "    cost = -1 / m * sum(error)\n",
    "    gradient = 1 / m * np.dot(x.transpose(), (y_pred - y))\n",
    "    return cost[0] , gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "45ac6567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initialization 0.693147180559946\n",
      "Gradient at initialization: [[-0.1       ]\n",
      " [-0.28122914]\n",
      " [-0.25098615]]\n",
      "Theta at initialization: [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mean_scores = np.mean(scores, axis=0)\n",
    "std_scores = np.std(scores, axis=0)\n",
    "scores = (scores - mean_scores) / std_scores #standardization\n",
    "\n",
    "rows = scores.shape[0]\n",
    "cols = scores.shape[1]\n",
    "\n",
    "X = np.append(np.ones((rows, 1)), scores, axis=1) #include intercept\n",
    "y = results.reshape(rows, 1)\n",
    "\n",
    "theta_init = np.zeros((cols + 1, 1))\n",
    "cost, gradient = compute_cost(theta_init, X, y)\n",
    "\n",
    "print(\"Cost at initialization\", cost)\n",
    "print(\"Gradient at initialization:\", gradient)\n",
    "print(\"Theta at initialization:\", theta_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b6e581d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(x, y, theta, alpha, iterations):\n",
    "    costs = []\n",
    "    for i in range(iterations):\n",
    "        #Random stochastic component (Taking only a subset of 50 samples each time)\n",
    "        idx = np.random.choice(np.arange(len(x)), 50, replace=False)\n",
    "        x_sample = X[idx]\n",
    "        y_sample = y[idx]\n",
    "        cost, gradient = compute_cost(theta, x_sample, y_sample)\n",
    "        theta -= (alpha * gradient)\n",
    "        costs.append(cost)\n",
    "        print(f'Iteration {i+1} \\t theta1 = {theta[1]} \\t theta2 = {theta[2]}')\n",
    "        #print(costs[-1])\n",
    "    #return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcb287db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \t theta1 = [4.09676355] \t theta2 = [3.6055308]\n",
      "Iteration 2 \t theta1 = [4.05776792] \t theta2 = [3.64534429]\n",
      "Iteration 3 \t theta1 = [4.06374609] \t theta2 = [3.63656884]\n",
      "Iteration 4 \t theta1 = [4.05833886] \t theta2 = [3.65860968]\n",
      "Iteration 5 \t theta1 = [4.03863486] \t theta2 = [3.66871013]\n",
      "Iteration 6 \t theta1 = [4.07823826] \t theta2 = [3.62704969]\n",
      "Iteration 7 \t theta1 = [4.06431299] \t theta2 = [3.65511741]\n",
      "Iteration 8 \t theta1 = [4.0623071] \t theta2 = [3.6708695]\n",
      "Iteration 9 \t theta1 = [4.0690008] \t theta2 = [3.66408104]\n",
      "Iteration 10 \t theta1 = [4.11312316] \t theta2 = [3.62536271]\n",
      "Iteration 11 \t theta1 = [4.10068279] \t theta2 = [3.62317717]\n",
      "Iteration 12 \t theta1 = [4.0862196] \t theta2 = [3.63639663]\n",
      "Iteration 13 \t theta1 = [4.08353922] \t theta2 = [3.64263138]\n",
      "Iteration 14 \t theta1 = [4.08360087] \t theta2 = [3.66287323]\n",
      "Iteration 15 \t theta1 = [4.09595297] \t theta2 = [3.61852852]\n",
      "Iteration 16 \t theta1 = [4.0649292] \t theta2 = [3.63893429]\n",
      "Iteration 17 \t theta1 = [4.08065677] \t theta2 = [3.62356109]\n",
      "Iteration 18 \t theta1 = [4.08072151] \t theta2 = [3.60356108]\n",
      "Iteration 19 \t theta1 = [4.03134258] \t theta2 = [3.64600662]\n",
      "Iteration 20 \t theta1 = [4.05135298] \t theta2 = [3.61941206]\n",
      "Iteration 21 \t theta1 = [4.04987452] \t theta2 = [3.62316083]\n",
      "Iteration 22 \t theta1 = [4.05184519] \t theta2 = [3.63079568]\n",
      "Iteration 23 \t theta1 = [4.01267495] \t theta2 = [3.65966495]\n",
      "Iteration 24 \t theta1 = [3.98326078] \t theta2 = [3.66764312]\n",
      "Iteration 25 \t theta1 = [3.94846883] \t theta2 = [3.72304945]\n",
      "Iteration 26 \t theta1 = [3.93080887] \t theta2 = [3.73751049]\n",
      "Iteration 27 \t theta1 = [3.94373229] \t theta2 = [3.7203195]\n",
      "Iteration 28 \t theta1 = [3.89383712] \t theta2 = [3.77474901]\n",
      "Iteration 29 \t theta1 = [3.85615293] \t theta2 = [3.81915707]\n",
      "Iteration 30 \t theta1 = [3.82740895] \t theta2 = [3.84331253]\n",
      "Iteration 31 \t theta1 = [3.81323187] \t theta2 = [3.85878556]\n",
      "Iteration 32 \t theta1 = [3.84714173] \t theta2 = [3.82697171]\n",
      "Iteration 33 \t theta1 = [3.85436748] \t theta2 = [3.83172398]\n",
      "Iteration 34 \t theta1 = [3.84583348] \t theta2 = [3.8335721]\n",
      "Iteration 35 \t theta1 = [3.78521338] \t theta2 = [3.90514973]\n",
      "Iteration 36 \t theta1 = [3.78652435] \t theta2 = [3.90161533]\n",
      "Iteration 37 \t theta1 = [3.83894548] \t theta2 = [3.85397443]\n",
      "Iteration 38 \t theta1 = [3.89159368] \t theta2 = [3.80300117]\n",
      "Iteration 39 \t theta1 = [3.88269731] \t theta2 = [3.80240056]\n",
      "Iteration 40 \t theta1 = [3.92524978] \t theta2 = [3.76758529]\n",
      "Iteration 41 \t theta1 = [3.91739778] \t theta2 = [3.77409134]\n",
      "Iteration 42 \t theta1 = [3.9602445] \t theta2 = [3.73856913]\n",
      "Iteration 43 \t theta1 = [3.96462618] \t theta2 = [3.73900622]\n",
      "Iteration 44 \t theta1 = [4.02589031] \t theta2 = [3.67981945]\n",
      "Iteration 45 \t theta1 = [3.95339171] \t theta2 = [3.75158295]\n",
      "Iteration 46 \t theta1 = [3.95303916] \t theta2 = [3.76165141]\n",
      "Iteration 47 \t theta1 = [3.91191764] \t theta2 = [3.80024335]\n",
      "Iteration 48 \t theta1 = [3.95530266] \t theta2 = [3.74472854]\n",
      "Iteration 49 \t theta1 = [3.97045102] \t theta2 = [3.71675735]\n",
      "Iteration 50 \t theta1 = [3.96967261] \t theta2 = [3.72535564]\n",
      "Iteration 51 \t theta1 = [3.99158437] \t theta2 = [3.71168146]\n",
      "Iteration 52 \t theta1 = [3.96587166] \t theta2 = [3.72990275]\n",
      "Iteration 53 \t theta1 = [3.93867707] \t theta2 = [3.74949626]\n",
      "Iteration 54 \t theta1 = [3.97961843] \t theta2 = [3.7159434]\n",
      "Iteration 55 \t theta1 = [3.9353119] \t theta2 = [3.75057981]\n",
      "Iteration 56 \t theta1 = [3.94119862] \t theta2 = [3.76359464]\n",
      "Iteration 57 \t theta1 = [3.93728988] \t theta2 = [3.77307306]\n",
      "Iteration 58 \t theta1 = [3.92463235] \t theta2 = [3.7718063]\n",
      "Iteration 59 \t theta1 = [3.92095838] \t theta2 = [3.7616277]\n",
      "Iteration 60 \t theta1 = [3.94714759] \t theta2 = [3.75230054]\n",
      "Iteration 61 \t theta1 = [3.91059391] \t theta2 = [3.78765695]\n",
      "Iteration 62 \t theta1 = [3.9687903] \t theta2 = [3.71426917]\n",
      "Iteration 63 \t theta1 = [3.94724204] \t theta2 = [3.74323504]\n",
      "Iteration 64 \t theta1 = [3.97850431] \t theta2 = [3.7226431]\n",
      "Iteration 65 \t theta1 = [3.98879056] \t theta2 = [3.71036249]\n",
      "Iteration 66 \t theta1 = [3.93369003] \t theta2 = [3.77875015]\n",
      "Iteration 67 \t theta1 = [3.91967646] \t theta2 = [3.78672033]\n",
      "Iteration 68 \t theta1 = [3.96237336] \t theta2 = [3.73738722]\n",
      "Iteration 69 \t theta1 = [4.01113222] \t theta2 = [3.69349478]\n",
      "Iteration 70 \t theta1 = [4.02277435] \t theta2 = [3.68616879]\n",
      "Iteration 71 \t theta1 = [4.01590997] \t theta2 = [3.68430671]\n",
      "Iteration 72 \t theta1 = [3.98963065] \t theta2 = [3.71517478]\n",
      "Iteration 73 \t theta1 = [4.00017473] \t theta2 = [3.68477904]\n",
      "Iteration 74 \t theta1 = [4.00400147] \t theta2 = [3.67273253]\n",
      "Iteration 75 \t theta1 = [4.0228499] \t theta2 = [3.65920963]\n",
      "Iteration 76 \t theta1 = [3.98486428] \t theta2 = [3.67519453]\n",
      "Iteration 77 \t theta1 = [3.9675995] \t theta2 = [3.6720299]\n",
      "Iteration 78 \t theta1 = [3.9085493] \t theta2 = [3.7538938]\n",
      "Iteration 79 \t theta1 = [3.89415636] \t theta2 = [3.75942585]\n",
      "Iteration 80 \t theta1 = [3.9493785] \t theta2 = [3.71581279]\n",
      "Iteration 81 \t theta1 = [3.94275406] \t theta2 = [3.7071696]\n",
      "Iteration 82 \t theta1 = [3.99310402] \t theta2 = [3.66762711]\n",
      "Iteration 83 \t theta1 = [4.01374281] \t theta2 = [3.64727848]\n",
      "Iteration 84 \t theta1 = [3.96117104] \t theta2 = [3.73230451]\n",
      "Iteration 85 \t theta1 = [3.9786337] \t theta2 = [3.7186481]\n",
      "Iteration 86 \t theta1 = [3.96248462] \t theta2 = [3.74144632]\n",
      "Iteration 87 \t theta1 = [3.97554068] \t theta2 = [3.7307442]\n",
      "Iteration 88 \t theta1 = [3.96607647] \t theta2 = [3.73005378]\n",
      "Iteration 89 \t theta1 = [3.96013078] \t theta2 = [3.74954354]\n",
      "Iteration 90 \t theta1 = [3.95996705] \t theta2 = [3.75718323]\n",
      "Iteration 91 \t theta1 = [3.94889668] \t theta2 = [3.76520421]\n",
      "Iteration 92 \t theta1 = [3.95923452] \t theta2 = [3.76430767]\n",
      "Iteration 93 \t theta1 = [3.98248909] \t theta2 = [3.75653217]\n",
      "Iteration 94 \t theta1 = [4.01210615] \t theta2 = [3.71309886]\n",
      "Iteration 95 \t theta1 = [3.96518101] \t theta2 = [3.74427948]\n",
      "Iteration 96 \t theta1 = [3.93109463] \t theta2 = [3.79154903]\n",
      "Iteration 97 \t theta1 = [3.92175124] \t theta2 = [3.80594204]\n",
      "Iteration 98 \t theta1 = [3.91179977] \t theta2 = [3.82981458]\n",
      "Iteration 99 \t theta1 = [3.93493458] \t theta2 = [3.81353078]\n",
      "Iteration 100 \t theta1 = [3.92500262] \t theta2 = [3.82158349]\n",
      "Iteration 101 \t theta1 = [3.90164069] \t theta2 = [3.83006749]\n",
      "Iteration 102 \t theta1 = [3.92193363] \t theta2 = [3.80583145]\n",
      "Iteration 103 \t theta1 = [3.90871418] \t theta2 = [3.82514328]\n",
      "Iteration 104 \t theta1 = [3.91078397] \t theta2 = [3.82116477]\n",
      "Iteration 105 \t theta1 = [3.91246121] \t theta2 = [3.848736]\n",
      "Iteration 106 \t theta1 = [3.93927792] \t theta2 = [3.82465547]\n",
      "Iteration 107 \t theta1 = [3.93658714] \t theta2 = [3.84382684]\n",
      "Iteration 108 \t theta1 = [3.89136917] \t theta2 = [3.88209887]\n",
      "Iteration 109 \t theta1 = [3.90711516] \t theta2 = [3.86528073]\n",
      "Iteration 110 \t theta1 = [3.93485553] \t theta2 = [3.83519165]\n",
      "Iteration 111 \t theta1 = [3.9758635] \t theta2 = [3.77894828]\n",
      "Iteration 112 \t theta1 = [3.96317593] \t theta2 = [3.77905122]\n",
      "Iteration 113 \t theta1 = [3.99310393] \t theta2 = [3.74641889]\n",
      "Iteration 114 \t theta1 = [4.01371019] \t theta2 = [3.71567228]\n",
      "Iteration 115 \t theta1 = [4.0122221] \t theta2 = [3.70987656]\n",
      "Iteration 116 \t theta1 = [3.99619975] \t theta2 = [3.72391345]\n",
      "Iteration 117 \t theta1 = [3.980392] \t theta2 = [3.7495196]\n",
      "Iteration 118 \t theta1 = [3.98638412] \t theta2 = [3.74991864]\n",
      "Iteration 119 \t theta1 = [3.97852055] \t theta2 = [3.75275306]\n",
      "Iteration 120 \t theta1 = [3.9837245] \t theta2 = [3.74226685]\n",
      "Iteration 121 \t theta1 = [3.98315546] \t theta2 = [3.73119388]\n",
      "Iteration 122 \t theta1 = [4.04234962] \t theta2 = [3.66791141]\n",
      "Iteration 123 \t theta1 = [4.03866657] \t theta2 = [3.67392034]\n",
      "Iteration 124 \t theta1 = [4.04423126] \t theta2 = [3.67121629]\n",
      "Iteration 125 \t theta1 = [4.01385989] \t theta2 = [3.70668389]\n",
      "Iteration 126 \t theta1 = [3.99659197] \t theta2 = [3.73623878]\n",
      "Iteration 127 \t theta1 = [3.96338243] \t theta2 = [3.78961154]\n",
      "Iteration 128 \t theta1 = [3.97230257] \t theta2 = [3.77377076]\n",
      "Iteration 129 \t theta1 = [4.02652464] \t theta2 = [3.70910519]\n",
      "Iteration 130 \t theta1 = [4.03743688] \t theta2 = [3.6903997]\n",
      "Iteration 131 \t theta1 = [3.98826803] \t theta2 = [3.74608207]\n",
      "Iteration 132 \t theta1 = [4.01104825] \t theta2 = [3.71385771]\n",
      "Iteration 133 \t theta1 = [4.00828522] \t theta2 = [3.70700292]\n",
      "Iteration 134 \t theta1 = [3.99646478] \t theta2 = [3.7062311]\n",
      "Iteration 135 \t theta1 = [4.01696264] \t theta2 = [3.69848013]\n",
      "Iteration 136 \t theta1 = [4.06255405] \t theta2 = [3.6563315]\n",
      "Iteration 137 \t theta1 = [4.05133017] \t theta2 = [3.6780431]\n",
      "Iteration 138 \t theta1 = [4.02373019] \t theta2 = [3.7160866]\n",
      "Iteration 139 \t theta1 = [4.03962029] \t theta2 = [3.68826765]\n",
      "Iteration 140 \t theta1 = [3.97702208] \t theta2 = [3.7395381]\n",
      "Iteration 141 \t theta1 = [3.99403517] \t theta2 = [3.71745361]\n",
      "Iteration 142 \t theta1 = [3.9815725] \t theta2 = [3.72594336]\n",
      "Iteration 143 \t theta1 = [4.01593031] \t theta2 = [3.69013967]\n",
      "Iteration 144 \t theta1 = [4.0558039] \t theta2 = [3.67118901]\n",
      "Iteration 145 \t theta1 = [4.07583403] \t theta2 = [3.65316516]\n",
      "Iteration 146 \t theta1 = [4.05602943] \t theta2 = [3.67186358]\n",
      "Iteration 147 \t theta1 = [4.07501509] \t theta2 = [3.63766976]\n",
      "Iteration 148 \t theta1 = [4.06731883] \t theta2 = [3.65245418]\n",
      "Iteration 149 \t theta1 = [4.05958541] \t theta2 = [3.68383136]\n",
      "Iteration 150 \t theta1 = [4.03066707] \t theta2 = [3.71637977]\n",
      "Iteration 151 \t theta1 = [4.02505766] \t theta2 = [3.6991805]\n",
      "Iteration 152 \t theta1 = [3.97831623] \t theta2 = [3.72915116]\n",
      "Iteration 153 \t theta1 = [4.03357448] \t theta2 = [3.66349192]\n",
      "Iteration 154 \t theta1 = [4.00908497] \t theta2 = [3.70698701]\n",
      "Iteration 155 \t theta1 = [3.96714798] \t theta2 = [3.75875029]\n",
      "Iteration 156 \t theta1 = [3.99237822] \t theta2 = [3.72668144]\n",
      "Iteration 157 \t theta1 = [3.99028994] \t theta2 = [3.73944837]\n",
      "Iteration 158 \t theta1 = [3.98559166] \t theta2 = [3.735829]\n",
      "Iteration 159 \t theta1 = [3.96763773] \t theta2 = [3.76636173]\n",
      "Iteration 160 \t theta1 = [3.99721753] \t theta2 = [3.72574312]\n",
      "Iteration 161 \t theta1 = [4.00508529] \t theta2 = [3.747314]\n",
      "Iteration 162 \t theta1 = [3.9945035] \t theta2 = [3.75852514]\n",
      "Iteration 163 \t theta1 = [3.97554247] \t theta2 = [3.77854576]\n",
      "Iteration 164 \t theta1 = [3.97073953] \t theta2 = [3.76627934]\n",
      "Iteration 165 \t theta1 = [3.93176716] \t theta2 = [3.7974582]\n",
      "Iteration 166 \t theta1 = [3.93369346] \t theta2 = [3.78706001]\n",
      "Iteration 167 \t theta1 = [3.96666655] \t theta2 = [3.76250184]\n",
      "Iteration 168 \t theta1 = [4.01498836] \t theta2 = [3.70535933]\n",
      "Iteration 169 \t theta1 = [4.04342588] \t theta2 = [3.67425076]\n",
      "Iteration 170 \t theta1 = [4.06862788] \t theta2 = [3.63951614]\n",
      "Iteration 171 \t theta1 = [4.06079121] \t theta2 = [3.64893266]\n",
      "Iteration 172 \t theta1 = [4.06275446] \t theta2 = [3.65951141]\n",
      "Iteration 173 \t theta1 = [4.0353413] \t theta2 = [3.68703759]\n",
      "Iteration 174 \t theta1 = [4.0213865] \t theta2 = [3.68539658]\n",
      "Iteration 175 \t theta1 = [4.0211107] \t theta2 = [3.66177791]\n",
      "Iteration 176 \t theta1 = [4.036365] \t theta2 = [3.64000903]\n",
      "Iteration 177 \t theta1 = [4.00634662] \t theta2 = [3.68704404]\n",
      "Iteration 178 \t theta1 = [4.00232396] \t theta2 = [3.69243221]\n",
      "Iteration 179 \t theta1 = [3.99703417] \t theta2 = [3.68112417]\n",
      "Iteration 180 \t theta1 = [4.06970741] \t theta2 = [3.62308868]\n",
      "Iteration 181 \t theta1 = [4.07685421] \t theta2 = [3.62115445]\n",
      "Iteration 182 \t theta1 = [4.08006159] \t theta2 = [3.61761617]\n",
      "Iteration 183 \t theta1 = [4.09551889] \t theta2 = [3.59636048]\n",
      "Iteration 184 \t theta1 = [4.05351775] \t theta2 = [3.65332507]\n",
      "Iteration 185 \t theta1 = [4.01721289] \t theta2 = [3.69634924]\n",
      "Iteration 186 \t theta1 = [4.04040103] \t theta2 = [3.66918219]\n",
      "Iteration 187 \t theta1 = [4.03714174] \t theta2 = [3.6595734]\n",
      "Iteration 188 \t theta1 = [3.99863925] \t theta2 = [3.69983701]\n",
      "Iteration 189 \t theta1 = [4.01044542] \t theta2 = [3.69462735]\n",
      "Iteration 190 \t theta1 = [4.03376679] \t theta2 = [3.6785011]\n",
      "Iteration 191 \t theta1 = [4.01545448] \t theta2 = [3.70691069]\n",
      "Iteration 192 \t theta1 = [4.03067336] \t theta2 = [3.67784906]\n",
      "Iteration 193 \t theta1 = [3.98869519] \t theta2 = [3.71720782]\n",
      "Iteration 194 \t theta1 = [3.977331] \t theta2 = [3.70449768]\n",
      "Iteration 195 \t theta1 = [3.99115371] \t theta2 = [3.67046849]\n",
      "Iteration 196 \t theta1 = [3.96653783] \t theta2 = [3.70207055]\n",
      "Iteration 197 \t theta1 = [3.95407524] \t theta2 = [3.71796914]\n",
      "Iteration 198 \t theta1 = [3.97460487] \t theta2 = [3.67415031]\n",
      "Iteration 199 \t theta1 = [3.97555855] \t theta2 = [3.65828634]\n",
      "Iteration 200 \t theta1 = [3.96282978] \t theta2 = [3.67857681]\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(X, y, theta_init, 1, 200)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
